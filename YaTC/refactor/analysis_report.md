# YaTC 论文分析报告

## 论文信息

- **标题**: Yet Another Traffic Classifier: A Masked Autoencoder Based Traffic Transformer with Multi-Level Flow Representation
- **发表**: AAAI 2023
- **作者**: Ruijie Zhao, Mingwei Zhan, Xianwen Deng, Yanhao Wang, Yijun Wang, Guan Gui, Zhi Xue

## 摘要

YaTC 是一个基于掩码自编码器（MAE）的流量 Transformer 模型，用于加密网络流量分类。该模型采用多级流表示（MFR）将网络流转换为图像格式，然后使用两阶段训练策略：先进行自监督预训练，再进行有监督微调。

## 核心创新点

### 1. 多级流表示（MFR）

MFR 是一种将网络流量转换为图像的方法：

- **流级别**: 每个流包含 5 个数据包
- **包级别**: 每个数据包提取 320 字节（80 字节头部 + 240 字节载荷）
- **字节级别**: 将字节值映射为灰度像素值

最终表示为 40×40 的灰度图像：
```
5 packets × 8 rows/packet = 40 rows
40 bytes/row = 40 columns
```

### 2. 两阶段训练策略

#### 预训练阶段（MAE）
- 使用掩码自编码器进行自监督学习
- 掩码比例：90%（远高于 ViT 的 75%）
- 目标：学习网络流量的通用表示

#### 微调阶段（TraFormer）
- 使用预训练的编码器
- 添加分类头进行有监督学习
- 采用逐层学习率衰减

### 3. 高掩码比例

论文发现 90% 的掩码比例对于流量分类任务效果最佳，这与图像任务（75%）不同。原因是：
- 网络流量具有高冗余性
- 相邻数据包之间存在强相关性
- 高掩码比例迫使模型学习更深层的语义特征

## 模型架构

### 编码器

| 参数 | 值 |
|------|-----|
| 嵌入维度 | 192 |
| Transformer 层数 | 4 |
| 注意力头数 | 16 |
| MLP 比率 | 4 |
| Patch 大小 | 2×2 |

### 解码器（仅预训练）

| 参数 | 值 |
|------|-----|
| 嵌入维度 | 128 |
| Transformer 层数 | 2 |
| 注意力头数 | 16 |
| MLP 比率 | 4 |

### Patch 嵌入

- 每个数据包图像大小：8×40
- Patch 大小：2×2
- 每个数据包的 Patch 数：(8/2) × (40/2) = 80
- 总 Patch 数：80 × 5 = 400

## 训练超参数

### 预训练

| 参数 | 值 |
|------|-----|
| 批量大小 | 128 |
| 基础学习率 | 1e-3 |
| 权重衰减 | 0.05 |
| 优化器 | AdamW (β₁=0.9, β₂=0.95) |
| 总步数 | 150,000 |
| 预热步数 | 10,000 |
| 学习率调度 | 余弦衰减 |

### 微调

| 参数 | 值 |
|------|-----|
| 批量大小 | 128 |
| 基础学习率 | 2e-3 |
| 权重衰减 | 0.05 |
| 优化器 | AdamW (β₁=0.9, β₂=0.999) |
| 总轮数 | 200 |
| 预热轮数 | 5 |
| 逐层学习率衰减 | 0.65 |
| 标签平滑 | 0.1 |

## 数据集

论文在四个数据集上进行了实验：

| 数据集 | 类别数 | 描述 |
|--------|--------|------|
| ISCXVPN2016 | 7 | VPN 流量分类 |
| ISCXTor2016 | 8 | Tor 流量分类 |
| USTC-TFC2016 | 20 | 恶意软件流量分类 |
| CICIoT2022 | 10 | IoT 设备流量分类 |

## 实验结果

YaTC 在所有数据集上都取得了最先进的性能：

| 数据集 | 准确率 | F1 分数 |
|--------|--------|---------|
| ISCXVPN2016 | 99.32% | 99.30% |
| ISCXTor2016 | 99.78% | 99.78% |
| USTC-TFC2016 | 99.59% | 99.59% |
| CICIoT2022 | 99.94% | 99.94% |

## 消融实验

### 掩码比例

| 掩码比例 | ISCXVPN2016 | ISCXTor2016 |
|----------|-------------|-------------|
| 0.5 | 98.12% | 99.21% |
| 0.75 | 98.89% | 99.54% |
| 0.9 | **99.32%** | **99.78%** |
| 0.95 | 99.01% | 99.67% |

### 预训练的重要性

| 方法 | ISCXVPN2016 | ISCXTor2016 |
|------|-------------|-------------|
| 从头训练 | 97.45% | 98.89% |
| 预训练 + 微调 | **99.32%** | **99.78%** |

## 结论

1. MFR 表示有效保留了网络流量的层次结构信息
2. MAE 预训练策略显著提升模型性能
3. 90% 的高掩码比例适合流量分类任务
4. 两阶段训练策略使模型能够泛��到不同的流量分类任务
